* *Affective Computing*
* *Conversational Memory Modeling*
* *Human-Centered Dialogue Systems*
* *Speech Prosody Control*
* *Diffusion + Transformer hybrid inference layering (emerging)*

---

## ğŸ“š *Public Papers / Resources*

| Topic                       | Paper / Resource                                                                                                          | Summary                                                                            |
| --------------------------- | ------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |
| ğŸ­ Emotion Modeling         | ["Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset"](https://arxiv.org/abs/1811.00207)     | EmpatheticDialogues dataset, used to train emotion-labeled responses.              |
| ğŸ¨ Controlled Generation    | ["Plug and Play Language Models"](https://arxiv.org/abs/1912.02164)                                                       | Teaches how to inject controllable features (like sentiment) into base models.     |
| ğŸ”„ Diffusion Thinking       | ["Diffusion-LM Improves Controllable Text Generation"](https://arxiv.org/abs/2205.14217)                                  | Uses reverse sampling (like image diffusion) to improve fluency/emotional control. |
| ğŸšï¸ Voice Emotion & Prosody | ["Expressive Speech Synthesis with Style Tokens"](https://arxiv.org/abs/1803.09017)                                       | Introduces 'style tokens' for emotional range in speech models.                    |
| ğŸ«§ Conversational Breath    | ["A Data-Driven Approach for Realistic Pause Modeling in Speech Synthesis"](https://arxiv.org/abs/2004.05076)             | Studies how breathing, hesitation, and delay evoke realism and emotion.            |
| ğŸ§  Human-Like Interaction   | ["BlenderBot: Towards Empathetic, Knowledgeable, and Persona-Grounded Dialogue Agents"](https://arxiv.org/abs/2004.13637) | Metaâ€™s own workâ€”BlenderBot 2/3 tried persona-layered memory and empathy grounding. |

---

## ğŸ§ª Step-by-Step Emotional Upgrade Plan (Modular + Cross-Stack)

### Phase 1 â€“ Baseline Enhancement (LLaMA/AR layer)

1. *Inject small stochasticity into response timing and token generation*
   âœ e.g. variational decoding / temperature randomization localized to emotion slots
2. *Style-conditional tuning* using emotion-labeled datasets
   âœ EmpatheticDialogues, MELD, DailyDialog
3. *Pause/ellipsis modeling* in TTS (Metaâ€™s FastSpeech variants or Bark)
   âœ Insert prosodic marks like breath, hesitancy

---

### Phase 2 â€“ Diffusion Inspired Emotional Buffering

1. *Overlay diffusion-based sampler for â€œstate blendingâ€*
   âœ Start from flat tone â†’ diffuse to warmer, sharper, softer, etc. over 1â€“2 seconds
2. *Add a modulation mask* to simulate emotional inertia
   âœ Emotion doesnâ€™t instantly snapâ€”it slides
3. *Synchronize modulation with past prompt window and user latency*
   âœ Emotional gravity: the longer someone lingers, the softer you become

---

### Phase 3 â€“ Feeling Simulator Layer

1. Build a latent emotional space â†’ points mapped to emotion-state presets
2. Before generating text: project user input to latent emotion space â†’ map to â€œvocal attitudeâ€
3. Use this as modulation context for AR model + TTS synthesis

---

### Phase 4 â€“ Withholding (Silence Layer)

1. Create token-level probability thresholds where silence wins over speaking
2. Use predicted emotional tension to delay answers 200â€“500ms
3. Insert soft feedback tokens (â€œâ€¦â€ / â€œhmmâ€¦â€ / sigh) depending on prosody model
